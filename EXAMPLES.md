# AI Patch Doctor - Example Outputs

This document shows example outputs for all `ai-patch doctor` command scenarios.

## Table of Contents

- [Overview](#overview)
- [Cost Check (`--target=cost`)](#cost-check---targetcost)
- [Streaming Check (`--target=streaming`)](#streaming-check---targetstreaming)
- [Retries Check (`--target=retries`)](#retries-check---targetretries)
- [Trace Check (`--target=trace`)](#trace-check---targettrace)
- [All Checks (`--target=all`)](#all-checks---targetall)
- [Interactive Mode (`-i`)](#interactive-mode--i)
- [CI Mode (`--ci`)](#ci-mode---ci)

---

## Overview

AI Patch Doctor supports multiple check targets:
- `streaming` - SSE/streaming issues (TTFB, chunk gaps)
- `retries` - Rate limiting and retry behavior
- `cost` - Token usage and cost estimation
- `trace` - Request traceability and correlation
- `all` - Run all checks (default)

Each check can have three status levels:
- âœ… **SUCCESS** - No issues detected
- âš ï¸ **WARNING** - Non-critical issues found
- âŒ **ERROR** - Critical issues detected

---

## Cost Check (`--target=cost`)

The cost check is informational-only and reports model pricing.

### Example: Success (Default)

```bash
$ ai-patch doctor --target=cost
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running cost checks...


âœ… Status: SUCCESS

Detected:
  â€¢ [cost] Model pricing: $0.5/1M input tokens, $1.5/1M output tokens

Not detected:
  â€¢ (No explicit checks for absent items in this run)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Different Models

```bash
$ ai-patch doctor --target=cost --model=gpt-4
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running cost checks...


âœ… Status: SUCCESS

Detected:
  â€¢ [cost] Model pricing: $30/1M input tokens, $60/1M output tokens

Not detected:
  â€¢ (No explicit checks for absent items in this run)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## Streaming Check (`--target=streaming`)

The streaming check probes SSE streaming performance.

### Example: Success (Fast TTFB)

When streaming is working well (TTFB < 5s):

```bash
$ ai-patch doctor --target=streaming
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running streaming checks...


âœ… Status: SUCCESS

Detected:
  â€¢ (No issues detected)

Not detected:
  â€¢ Streaming stalls (TTFB < 5s)
  â€¢ Chunk gaps (max gap < 10s)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Warning (Slow TTFB)

When streaming has a slow time-to-first-byte (TTFB > 5s):

```bash
$ ai-patch doctor --target=streaming
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://aibadgr.com/v1
âœ“ Provider: openai-compatible

ğŸ”¬ Running streaming checks...


âš ï¸ Status: WARNING

Detected:
  â€¢ [streaming] TTFB: 6.7s (threshold: 5s)

Not detected:
  â€¢ Chunk gaps (max gap < 10s threshold)

Not observable from provider probe:
  â€¢ Whether client retries after partial stream
  â€¢ Stream truncation behavior

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [streaming] TTFB: 6.7s (threshold: 5s)

What I can't see: Whether client retries after partial stream

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://api.openai.com"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Error (Chunk Gap)

When there are large gaps between streaming chunks (> 30s):

```bash
$ ai-patch doctor --target=streaming
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.custom-provider.com/v1
âœ“ Provider: openai-compatible

ğŸ”¬ Running streaming checks...


âŒ Status: ERROR

Detected:
  â€¢ [streaming] Max chunk gap: 35.2s (>30s threshold)
  â€¢ [streaming] TTFB: 7.1s (threshold: 5s)

Not detected:
  â€¢ (No checks passed)

Not observable from provider probe:
  â€¢ Whether client retries after partial stream
  â€¢ Stream truncation behavior
  â€¢ Client-side timeout handling

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [streaming] Max chunk gap: 35.2s (>30s threshold)

What I can't see: Whether client retries after partial stream

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://api.custom-provider.com/v1"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## Retries Check (`--target=retries`)

The retries check probes for rate limiting and retry behavior.

### Example: Success (No Rate Limiting)

When no rate limiting is detected:

```bash
$ ai-patch doctor --target=retries
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running retries checks...


âœ… Status: SUCCESS

Detected:
  â€¢ (No issues detected)

Not detected:
  â€¢ Rate limiting (no 429s in 1 probe)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Warning (Rate Limiting Detected)

When rate limiting (HTTP 429) is encountered:

```bash
$ ai-patch doctor --target=retries
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running retries checks...


âš ï¸ Status: WARNING

Detected:
  â€¢ [retries] Rate limiting detected (HTTP 429)
  â€¢ [retries] Retry-After header: 20s

Not detected:
  â€¢ (Rate limiting was detected)

Not observable from provider probe:
  â€¢ Retry policy
  â€¢ Retry after stream start

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [retries] Rate limiting detected (HTTP 429)

What I can't see: Retry policy

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://api.openai.com"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Warning (Low Rate Limit Remaining)

When rate limit is low but not exhausted:

```bash
$ ai-patch doctor --target=retries
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running retries checks...


âš ï¸ Status: WARNING

Detected:
  â€¢ [retries] Rate limit remaining: 5 requests

Not detected:
  â€¢ Rate limiting (no 429s in 1 probe)

Not observable from provider probe:
  â€¢ Retry policy
  â€¢ Retry after stream start

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [retries] Rate limit remaining: 5 requests

What I can't see: Retry policy

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://api.openai.com"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## Trace Check (`--target=trace`)

The trace check validates request traceability.

### Example: Success (Provider Request ID Found)

When the provider returns request IDs:

```bash
$ ai-patch doctor --target=trace
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running trace checks...


âœ… Status: SUCCESS

Detected:
  â€¢ [trace] Provider request ID: req_abc123xyz789
  â€¢ [trace] Generated request hash: 29fb36cbe1742ae3

Not detected:
  â€¢ (No issues detected)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Warning (No Provider Request ID)

When the provider doesn't return request IDs:

```bash
$ ai-patch doctor --target=trace
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://aibadgr.com/v1
âœ“ Provider: openai-compatible

ğŸ”¬ Running trace checks...


âš ï¸ Status: WARNING

Detected:
  â€¢ [trace] Provider request ID not found in response headers
  â€¢ [trace] Generated request hash: 29fb36cbe1742ae3

Not detected:
  â€¢ Provider request ID (not found in response headers)

Not observable from provider probe:
  â€¢ Duplicate requests

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [trace] Provider request ID not found in response headers

What I can't see: Duplicate requests

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://aibadgr.com/v1"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## All Checks (`--target=all`)

The default target that runs all checks.

### Example: Success (All Checks Pass)

When all checks pass:

```bash
$ ai-patch doctor --target=all
# or simply
$ ai-patch doctor
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running all checks...


âœ… Status: SUCCESS

Detected:
  â€¢ [cost] Model pricing: $0.5/1M input tokens, $1.5/1M output tokens
  â€¢ [trace] Provider request ID: req_abc123xyz789
  â€¢ [trace] Generated request hash: 29fb36cbe1742ae3

Not detected:
  â€¢ Rate limiting (no 429s in 1 probe)
  â€¢ Streaming stalls (TTFB < 5s)
  â€¢ Chunk gaps (max gap < 10s)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Warning (Mixed Results)

When some checks have warnings:

```bash
$ ai-patch doctor --target=all
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://aibadgr.com/v1
âœ“ Provider: openai-compatible

ğŸ”¬ Running all checks...


âš ï¸ Status: WARNING

Detected:
  â€¢ [streaming] TTFB: 6.7s (threshold: 5s)
  â€¢ [cost] Model pricing: $0.5/1M input tokens, $1.5/1M output tokens
  â€¢ [trace] Provider request ID not found in response headers
  â€¢ [trace] Generated request hash: 29fb36cbe1742ae3

Not detected:
  â€¢ Rate limiting (no 429s in 1 probe)
  â€¢ Provider request ID (not found in response headers)

Not observable from provider probe:
  â€¢ Whether client retries after partial stream
  â€¢ Duplicate requests

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [trace] Provider request ID not found in response headers

What I can't see: Whether client retries after partial stream

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://aibadgr.com/v1"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: Error (Critical Issues)

When critical issues are detected:

```bash
$ ai-patch doctor --target=all
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.custom-provider.com/v1
âœ“ Provider: openai-compatible

ğŸ”¬ Running all checks...


âŒ Status: ERROR

Detected:
  â€¢ [streaming] Max chunk gap: 35.2s (>30s threshold)
  â€¢ [streaming] TTFB: 7.1s (threshold: 5s)
  â€¢ [retries] Rate limiting detected (HTTP 429)
  â€¢ [cost] Model pricing: $0.5/1M input tokens, $1.5/1M output tokens
  â€¢ [trace] Provider request ID not found in response headers
  â€¢ [trace] Generated request hash: 29fb36cbe1742ae3

Not detected:
  â€¢ Provider request ID (not found in response headers)

Not observable from provider probe:
  â€¢ Whether client retries after partial stream
  â€¢ Retry policy
  â€¢ Duplicate requests
  â€¢ Stream truncation behavior

Note:
Here's exactly what I can see from the provider probe.
Here's what I cannot see without real traffic.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

============================================================

What I found: [streaming] Max chunk gap: 35.2s (>30s threshold)

What I can't see: retry behavior, partial streams, concurrency

Run one request through Badgr gateway (copy/paste):

export OPENAI_BASE_URL="https://aibadgr.com/v1"
# Make one API call here (your code)
export OPENAI_BASE_URL="https://api.custom-provider.com/v1"

============================================================

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## Interactive Mode (`-i`)

Interactive mode prompts for target and provider selection.

### Example: Interactive Flow

```bash
$ ai-patch doctor -i
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch

ğŸ” AI Patch Doctor - Interactive Mode

What's failing?
  1. streaming / SSE stalls / partial output
  2. retries / 429 / rate-limit chaos
  3. cost spikes
  4. traceability (request IDs, duplicates)
  5. prod-only issues (all checks)
Select [1-5, default: 5]: 1

What do you use?
  1. openai-compatible (default)
  2. anthropic
  3. gemini
Select [1-3, default: 1]: 1


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running streaming checks...


âœ… Status: SUCCESS

Detected:
  â€¢ (No issues detected)

Not detected:
  â€¢ Streaming stalls (TTFB < 5s)
  â€¢ Chunk gaps (max gap < 10s)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## CI Mode (`--ci`)

CI mode disables all prompts and fails fast on missing configuration.

### Example: CI Mode with API Key

```bash
$ export OPENAI_API_KEY="sk-..."
$ ai-patch doctor --ci
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.openai.com
âœ“ Provider: openai-compatible

ğŸ”¬ Running all checks...


âœ… Status: SUCCESS

Detected:
  â€¢ [cost] Model pricing: $0.5/1M input tokens, $1.5/1M output tokens
  â€¢ [trace] Provider request ID: req_abc123xyz789
  â€¢ [trace] Generated request hash: 29fb36cbe1742ae3

Not detected:
  â€¢ Rate limiting (no 429s in 1 probe)
  â€¢ Streaming stalls (TTFB < 5s)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Example: CI Mode without API Key (Failure)

```bash
$ unset OPENAI_API_KEY
$ ai-patch doctor --ci
```

**Output:**
```
âŒ Missing configuration: OPENAI_API_KEY
   Set environment variable(s) or run with -i for interactive mode
```

**Exit Code:** 2

---

## Different Providers

### Anthropic Claude

```bash
$ ai-patch doctor --provider=anthropic --target=streaming
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://api.anthropic.com
âœ“ Provider: anthropic

ğŸ”¬ Running streaming checks...


âœ… Status: SUCCESS

Detected:
  â€¢ (No issues detected)

Not detected:
  â€¢ Streaming stalls (TTFB < 5s)
  â€¢ Chunk gaps (max gap < 10s)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

### Google Gemini

```bash
$ ai-patch doctor --provider=gemini --target=cost
```

**Output:**
```
Generated by AI Patch â€” re-run: npx ai-patch


âœ“ Detected: https://generativelanguage.googleapis.com
âœ“ Provider: gemini

ğŸ”¬ Running cost checks...


âœ… Status: SUCCESS

Detected:
  â€¢ [cost] Model pricing: $0.5/1M input tokens, $1.5/1M output tokens

Not detected:
  â€¢ (No explicit checks for absent items in this run)

All checks passed for this run. This tool does not monitor production.

ğŸ“Š Report: ./ai-patch-reports/latest/report.md

---
â„¹ï¸  This report explains this incident only.

If this happens again in production, you won't see it unless you run this manually.

Generated by AI Patch â€” re-run: npx ai-patch
```

---

## Summary of Exit Codes

- **0** - Success (all checks passed or warnings only)
- **1** - Errors detected (critical issues found)
- **2** - Configuration error or invalid arguments

---

## Notes

1. **"Generated by AI Patch â€” re-run: npx ai-patch"** appears at the start and end of each run for easy re-execution.

2. **Status levels**:
   - âœ… SUCCESS - No issues or only informational findings
   - âš ï¸ WARNING - Non-critical issues detected
   - âŒ ERROR - Critical issues found

3. **Badgr messaging** only appears when status is not SUCCESS, suggesting using the Badgr gateway for deeper diagnostics.

4. **Not observable** items only appear when there are warnings/errors, indicating limitations of the synthetic probe.

5. All reports are saved to `./ai-patch-reports/latest/` with both JSON and Markdown formats.
